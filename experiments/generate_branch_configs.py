"""
Generate configuration files for branched training experiment.

This script creates a separate config file for each k-value in the CBS experiment.
Each config specifies:
- Checkpoint to load from (the base checkpoint)
- Scaled batch size (k * BASE_BATCH_SIZE)
- Scaled learning rate (sqrt(k) * BASE_LR)
- Output directory for results
"""

import os
import math

# =============================================================================
# EXPERIMENT CONFIGURATION - MODIFY THESE VALUES
# =============================================================================

# Base checkpoint to branch from
# THIS IS THE CHECKPOINT ALL BRANCHES WILL START FROM
BASE_CHECKPOINT_DIR = "out_trial_45m"  # Directory containing the base checkpoint
BASE_CHECKPOINT_FILE = "ckpt.pt"       # Checkpoint filename

# Base hyperparameters (from your original training run)
BASE_BATCH_SIZE = 8          # This is B in the paper
BASE_LEARNING_RATE = 3e-4    # This is eta in the paper
BASE_BLOCK_SIZE = 128        # Sequence length

# Experiment parameters
K_VALUES = [1, 2, 4, 8, 16, 32]      # k values to test
DELTA_STEPS_AS_TOKENS = 100_000      # Number of tokens to train each branch
                                      # (will be converted from steps)

# Data paths
TRAIN_DATA_FILE = 'data/shakespeare_char/train.bin'
VAL_DATA_FILE = 'data/shakespeare_char/val.bin'

# Optimizer settings
OPTIMIZER_TYPE = "adamw"
WEIGHT_DECAY = 0.1
BETA1 = 0.9
BETA2 = 0.95

# Logging and checkpointing
LOG_INTERVAL_TOKENS = 10_000
CHECKPOINT_INTERVAL_TOKENS = 50_000
EVAL_ITERS = 50

# WandB settings
WANDB_LOG = True
WANDB_PROJECT = "branched-training-cbs"

# Output directory for configs
CONFIG_OUTPUT_DIR = "configs_branch_sweep"

# =============================================================================
# END OF CONFIGURATION
# =============================================================================


def generate_config_file(k: int, output_dir: str):
    """
    Generate a config file for a specific k-value.
    
    Args:
        k: The scaling factor (batch_size = k * BASE_BATCH_SIZE)
        output_dir: Directory to save the config file
    """
    # Calculate scaled hyperparameters
    scaled_batch_size = k * BASE_BATCH_SIZE
    f_k = math.sqrt(k)  # f(k) = sqrt(k) for Adam/AdamW
    scaled_lr = f_k * BASE_LEARNING_RATE
    
    # Calculate max_tokens from DELTA_STEPS
    # We want to train for a specific number of tokens, not steps
    max_tokens = DELTA_STEPS_AS_TOKENS
    
    # Create output directory name for this branch
    branch_out_dir = f"out_branch_k{k}"
    
    # Create WandB run name
    wandb_run_name = f"branch_k{k}_B{scaled_batch_size}_lr{scaled_lr:.2e}"
    
    # Generate config content
    config_content = f'''# Configuration for Branch k={k}
# Generated by generate_branch_configs.py
# 
# This branch trains with:
#   - Batch size: {scaled_batch_size} (k={k} × BASE_BATCH_SIZE={BASE_BATCH_SIZE})
#   - Learning rate: {scaled_lr:.6f} (sqrt({k}) × BASE_LR={BASE_LEARNING_RATE})
#   - Training tokens: {max_tokens:,}

# I/O
out_dir = '{branch_out_dir}'
init_from = 'resume'
resume_ckpt_fname = '../{BASE_CHECKPOINT_DIR}/{BASE_CHECKPOINT_FILE}'  # Path to base checkpoint
log_interval_tokens = {LOG_INTERVAL_TOKENS}
checkpoint_interval_tokens = {CHECKPOINT_INTERVAL_TOKENS}
eval_iters = {EVAL_ITERS}

# WandB logging
wandb_log = {WANDB_LOG}
wandb_project = '{WANDB_PROJECT}'
wandb_run_name = '{wandb_run_name}'

# Data
train_data_file = '{TRAIN_DATA_FILE}'
val_data_file = '{VAL_DATA_FILE}'
block_size = {BASE_BLOCK_SIZE}
checkpoint_token_pos = 0  # Start from beginning of checkpoint
branch_seed = -1
branch_window_size_tokens = 100000000

# Training
batch_size = {scaled_batch_size}  # k={k} x BASE_BATCH_SIZE={BASE_BATCH_SIZE}
gradient_accumulation_steps = 1
max_tokens = {max_tokens}  # Train for {max_tokens:,} tokens

# Optimizer
optimizer_type = "{OPTIMIZER_TYPE}"
learning_rate = {scaled_lr}  # sqrt({k}) x BASE_LR={BASE_LEARNING_RATE}
weight_decay = {WEIGHT_DECAY}
beta1 = {BETA1}
beta2 = {BETA2}
grad_clip = 1.0
lr_muon_adam = {scaled_lr}  # Same as learning_rate for consistency
wd_muon_adam = {WEIGHT_DECAY}

# Branch metadata (for tracking)
# k_value = {k}
# base_batch_size = {BASE_BATCH_SIZE}
# base_learning_rate = {BASE_LEARNING_RATE}
# scaling_factor_fk = {f_k}
'''
    
    # Save config file
    config_filename = f"config_branch_k{k}.py"
    config_path = os.path.join(output_dir, config_filename)
    
    with open(config_path, 'w') as f:
        f.write(config_content)
    
    print(f"  ✓ Generated config for k={k:2d}: {config_filename}")
    print(f"    Batch size: {scaled_batch_size:3d}, LR: {scaled_lr:.6f}, Tokens: {max_tokens:,}")


def main():
    """Generate all config files for the branched training experiment."""
    
    print("="*70)
    print("Branched Training Config Generator")
    print("="*70)
    print()
    print("Base Configuration:")
    print(f"  Base checkpoint: {BASE_CHECKPOINT_DIR}/{BASE_CHECKPOINT_FILE}")
    print(f"  Base batch size (B): {BASE_BATCH_SIZE}")
    print(f"  Base learning rate (eta): {BASE_LEARNING_RATE}")
    print(f"  Block size: {BASE_BLOCK_SIZE}")
    print(f"  Training tokens per branch: {DELTA_STEPS_AS_TOKENS:,}")
    print()
    print(f"K values to test: {K_VALUES}")
    print()
    
    # Create output directory for configs
    os.makedirs(CONFIG_OUTPUT_DIR, exist_ok=True)
    print(f"Saving configs to: {CONFIG_OUTPUT_DIR}/")
    print()
    
    # Generate config for each k value
    print("Generating configs:")
    for k in K_VALUES:
        generate_config_file(k, CONFIG_OUTPUT_DIR)
    
    print()
    print("="*70)
    print("Config generation complete!")
    print("="*70)
    print()
    print("Next steps:")
    print(f"  1. Review the configs in '{CONFIG_OUTPUT_DIR}/'")
    print(f"  2. Ensure base checkpoint exists at: {BASE_CHECKPOINT_DIR}/{BASE_CHECKPOINT_FILE}")
    print(f"  3. Run: python run_branch_sweep.py")
    print()


if __name__ == "__main__":
    main()