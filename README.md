# Critical Batch Size Study (nanoGPT-45M)

This repository contains the **reproducible implementation** for our Critical Batch Size (CBS) study on modern optimizers (AdamW vs Muon) using a custom **45 million-parameter nanoGPT** model.

This repo provides a **minimal, importable nanoGPT core** that supports:
- Lightweight 45M model (`model_45M.py`)
- Shared dataset and loader setup
- Branching experiment loop (CBS measurement)
- Large-scale training and checkpointing loop

It is collaboratively developed by:
- **Sammie Smith** – Model setup (45 M, training smoke test), data analysis and figure generation codes for the paper
- **Liz Weaver** – Dataset preparation (random subset, 20B-token limit)
- **Beth Polito** – Branching experiment logic
- **Kithmini Herath** – Hyperparameter sweep setup, Large training + checkpointing system

---

## Repository Structure
```bash
cs182-cbs/
│
├── config/                                   # Example training configuration files
│   ├── train_shakespeare_char.py             # initial training config file for dummy dataset - shakespeare char
│   ├── train_sweep_muon_configs.py           # example configuration file for the hyperparameter sweep experiment
│   └── train_large_muon_scheduler_configs.py # example configuration file for the long training run
│
├── data/                    # Data preparation and loading
│   ├── c4_dataset/          # The main dataset directory. Contains the data and preparation scripts used for the hyperparameter sweep, long training run and branching experiments.
│   ├── shakespeare_char/    # The dummy dataset directory.
│
├── experiments/             # Branching CBS experiments
│   ├── branching.sh         # Example shell script for running the branching experiments
│   ├── generate_multi_checkpoint_configs.py # Script for generating multiple checkpoint configurations
│   ├── run_multi_branch_sweep.py # Script for running branching experiments using config files generated by generate_multi_checkpoint_configs.py
│   └── ...                  # Branching logic test scripts
│
├── utils/                   # Utility functions
│   ├── sample.py
│   ├── token_counter.py
│
├── model.py                 # Core GPT architecture (from nanoGPT)
├── model_45M.py             # Defines 45M-param model (importable)
├── train_trial_45M.py       # Quick smoke test for training
│
├── dataloader.py            # Data loading utilities
├── test_dataloader_comprehensive.py # Dataloader testing codes
├── optimizers.py            # Optimizer implementations (AdamW, Muon)
├── configurator.py          # Loads config files
├── requirements.txt         # Project dependencies
│
├── train.py                 # Main training code (for the long run and branching experiments)
├── train_sweep.py           # Hyperparameter sweep training script
│
├── cbs_analysis.py          # Script to calculate CBS values from log files
├── cbs_plot.py              # Generate CBS comparison plots
├── simple_log_plotter.py    # Extract and visualize training/validation results
├── logs_branch_multi_adam.zip # AdamW training logs
├── logs_branch_multi_muon.zip # Muon training logs
│
├── optimizer_sweep_experiment.md # Optimizer sweep experiment README
├── cbs_main_train_experiment.md # Long training run README
├── Data_analysis_and_plottingREADME.md # Data analysis and plotting README
└── README.md                # This file
```



---

## Environment Setup (Conda)

> We recommend using **Python 3.11+** with a dedicated conda environment.

```bash
# 1) Create and activate the environment
conda create -n cbs-nanogpt python=3.11 -y
conda activate cbs-nanogpt

# 2) Install PyTorch 2.9
# Select the installation option based on your CUDA version on Linux/Windows systems:
# a) CUDA 12.6
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126
# or
# b) CUDA 12.8
pip3 install torch torchvision
# or
# c) CUDA 13.0
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu130

# ---- OR CPU-only build (for Linux) ----
# pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# 3) Install project dependencies
pip install numpy tqdm tiktoken datasets wandb matplotlib
```

## Verify Installation

Clone the repository and run a quick smoke test

```bash
git clone https://github.com/bethpol/cs182-cbs.git
cd cs182-cbs
python train_trial_45M.py
```
You should see an output similar to:

```bash
Params: 46,764,000
step   20 | train loss 3.25 | val loss 3.29
step   40 | train loss 3.18 | val loss 3.25
...
Saved out_trial_45m/ckpt.pt
```

This confirms that the model builds correctly (~45M parameters), training and validation losses decrease, and dependencies are working across OS/PyTorch versions


## Trial Training

To verify that the model trains end to end, use a tiny shakespeare char dataset
```bash
# Prepare data
python data/shakespeare_char/prepare.py # this creates train.bin, val.bin. I have already generated these for convenience, so you can skip this step.

# Run quick training (≈1–3 min on GPU, 10–15 min on CPU)
python train_trial_45M.py
```


## Model Overview (model_45M.py)
```bash
from model import GPT, GPTConfig

DEFAULT_45M_CFG = dict(
    block_size=1024,
    vocab_size=50304,
    n_layer=8,
    n_head=12,
    n_embd=480,
    bias=False,
    dropout=0.0,
)

def build_gpt_45m(device="cuda"):
    cfg = GPTConfig(**DEFAULT_45M_CFG)
    model = GPT(cfg)
    return model.to(device)
```
To import and use in any script:

```bash
from model_45M import build_gpt_45m
model = build_gpt_45m(device="cuda")
```

## Data Overview (data/)

We use a subset of the C4 dataset. We have two separate subsets:
1. ```data/c4_dataset/100M``` - A 100M token subset for hyperparameter tuning.
2. ```data/c4_dataset/5B``` - A 5B token dataset for our long training run and branching experiments. Please note that this dataset is large and requires Git LFS to access.

To access the ```data/c4_dataset/5B```:

```bash
# Install Git LFS
brew install git-lfs

# Initialize Git LFS in the repo
git lfs install

# Pull the actual data files
git lfs pull

# Verify the files are fully downloaded by checking file sizes
ls -lh data/c4_dataset/5B # ~19GB
```

## Training

We have three main experimental setups:

1. **Hyperparameter sweeps** - We used the ```train_sweep.py``` script for this. For details on how to conduct this experiment please refer ```optimizer_sweep_experiment.md```
2. **Long training run and checkpointing** - We used the ```train.py``` script. For details on how to conduct this experiment please refer ```cbs_main_train_experiment.md```.
3. **Branching experiments** - We used the ```experiments/run_multi_branch_sweep.py``` script. This script will access the configuration files generated by ```experiments/generate_multi_checkpoint_configs.py``` and use ```train.py``` script for training and checkpointing. The experiments for each configuration will be launched sequentially.

## Data Analysis and Figure Generation for the Paper

Please refer ```Data_analysis_and_plottingREADME.md``` for more details.